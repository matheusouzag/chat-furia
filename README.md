# WebChat FURIA

## üöÄ Funcionalidades Implementadas

1. **Interface de Chat**:
   - Permite interagir com o Grupo Furioso (com funcionalidade de loop de conversas).
   - Criar conversa entre o usu√°rio e um assistente virtual, caracterizado pelos jogadores da FURIA. (Bot√£o "+ Converse com os jogadores") 
   - Respostas do assistente customizadas com base no jogador, utilizando o LLM LLaMA3 localmente.

2. **Tecnologias Utilizadas**:
   - **Next.js**: Framework React para desenvolvimento escal√°vel.
   - **TypeScript**: Para c√≥digo mais seguro e gerenci√°vel.
   - **Tailwind CSS** e **shadcn/ui**: Para estiliza√ß√£o moderna e componentes reutiliz√°veis.
   - **LLaMA3**: o Ollama LLaMA 3 √© um modelo de linguagem.
---

## üõ†Ô∏è Instala√ß√£o e Execu√ß√£o

### Pr√©-requisitos
Ter instalado:
- **Node.js**
- **npm** ou **yarn**
- **Ollama**

### Passos para Configura√ß√£o
1. **Clone o reposit√≥rio**:
   ```bash
   git clone https://github.com/matheusouzag/chat-furia.git

2. **Instale as depend√™ncias:**:
   ```bash
   npm install

3. **Instala√ß√£o do Ollama**:
   Acessando o link: https://ollama.com/download
   Ap√≥s isso basta abrir o terminal e executar o seguinte comando (o modelo possui 5GB):
   ```bash
   ollama pull llama3

4. **Inicie o localhost (Abra http://localhost:3000 no navegador)**:
   ```bash
   npm run dev

## üß± Arquitetura do Projeto

O projeto foi estruturado garantindo escalabilidade e organiza√ß√£o, seguindo a seguinte estrutura:

1. **public/images**: Para organiza√ß√£o de imagens
2. **src/app**: Contendo a estrutura principal do chat
3. **src/components**: Com os seguintes Componentes utilizados:
   - Contacts: Para os Contatos
   - Control: Para bot√µes utilizados
   - Header/Footer: Para complementar o chat
   - Message: Para os bal√µes de texto
4. **src/pages/api/chat.ts**: Local que esta configurado nosso modelo.
